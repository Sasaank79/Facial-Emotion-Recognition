{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ðŸŽ­ Facial Emotion Recognition v2.0 - Colab Training\n",
        "\n",
        "**Architecture**: EfficientNetV2-S | **Dataset**: FER-2013 | **GPU**: T4\n",
        "\n",
        "## What This Notebook Does\n",
        "\n",
        "1. âœ… Mounts Google Drive (models auto-save)\n",
        "2. âœ… Creates project structure locally (fast)\n",
        "3. âœ… Trains with SOTA techniques\n",
        "4. âœ… Saves results to your Drive\n",
        "\n",
        "**Storage**: Training runs locally (fast), models save to Drive (persistent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## ðŸ”§ Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print('âœ… Drive mounted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu_check"
      },
      "outputs": [],
      "source": [
        "# Verify GPU\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f'âœ… GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
        "else:\n",
        "    print('âŒ No GPU! Go to Runtime > Change runtime type > T4 GPU')\n",
        "    raise RuntimeError('GPU required for training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q timm==0.9.12 albumentations==1.3.1 onnx==1.15.0\n",
        "print('âœ… Dependencies installed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_setup"
      },
      "source": [
        "## ðŸ“‚ Project Setup with Drive Symlinks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_structure"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Create Drive folders (persistent)\n",
        "drive_base = Path('/content/drive/MyDrive/Emotion_Detection_v2')\n",
        "drive_models = drive_base / 'models'\n",
        "drive_results = drive_base / 'results'\n",
        "drive_models.mkdir(parents=True, exist_ok=True)\n",
        "drive_results.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Create local project\n",
        "project_dir = Path('/content/emotion_detection_v2')\n",
        "project_dir.mkdir(exist_ok=True)\n",
        "(project_dir / 'src' / 'models').mkdir(parents=True, exist_ok=True)\n",
        "(project_dir / 'src' / 'data').mkdir(parents=True, exist_ok=True)\n",
        "(project_dir / 'src' / 'utils').mkdir(parents=True, exist_ok=True)\n",
        "(project_dir / 'data').mkdir(exist_ok=True)\n",
        "\n",
        "# Create symlinks to Drive\n",
        "local_models = project_dir / 'models'\n",
        "local_results = project_dir / 'results'\n",
        "if local_models.exists() and local_models.is_symlink():\n",
        "    local_models.unlink()\n",
        "if local_results.exists() and local_results.is_symlink():\n",
        "    local_results.unlink()\n",
        "local_models.symlink_to(drive_models)\n",
        "local_results.symlink_to(drive_results)\n",
        "\n",
        "print('âœ… Project structure created')\n",
        "print(f'   Models â†’ Drive: {drive_models}')\n",
        "print(f'   Results â†’ Drive: {drive_results}')\n",
        "print('\\nðŸ’¡ All models and results will auto-save to your Google Drive!')\n",
        "\n",
        "%cd /content/emotion_detection_v2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_section"
      },
      "source": [
        "## ðŸ“¥ Load Dataset\n",
        "\n",
        "**Choose ONE option below:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_kaggle"
      },
      "outputs": [],
      "source": [
        "# Option 1: Kaggle Download (requires kaggle.json)\n",
        "# Upload your kaggle.json to Drive first, then uncomment:\n",
        "\n",
        "# !mkdir -p ~/.kaggle\n",
        "# !cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "# !kaggle datasets download -d msambare/fer2013 -p data/ --unzip\n",
        "# print('âœ… FER-2013 downloaded from Kaggle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_drive"
      },
      "outputs": [],
      "source": [
        "# Option 2: Copy from your Drive (if you already have it)\n",
        "# Uncomment if your data is already in Drive:\n",
        "\n",
        "import shutil\n",
        "drive_data = '/content/drive/MyDrive/Facial_Emotion_Detection/data'\n",
        "if Path(drive_data).exists():\n",
        "    print('Copying data from Drive...')\n",
        "    shutil.copytree(drive_data, 'data', dirs_exist_ok=True)\n",
        "    print('âœ… Data copied from Drive')\n",
        "else:\n",
        "    print('âŒ Data not found in Drive. Use Option 1 or check path.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "verify_data"
      },
      "outputs": [],
      "source": [
        "# Verify data structure\n",
        "!ls -lh data/\n",
        "!ls data/train/ 2>/dev/null || ls data/Test/ 2>/dev/null || echo 'âš ï¸ Check data structure'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "code_section"
      },
      "source": [
        "## ðŸ“ Create Source Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_files"
      },
      "outputs": [],
      "source": [
        "# Create package files\n",
        "!touch src/__init__.py src/models/__init__.py src/data/__init__.py src/utils/__init__.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_code"
      },
      "outputs": [],
      "source": [
        "%%writefile src/models/efficientnet.py\n",
        "\"\"\"EfficientNetV2 for emotion recognition\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "\n",
        "class EmotionEfficientNet(nn.Module):\n",
        "    def __init__(self, model_name='tf_efficientnetv2_s', num_classes=7, pretrained=True, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n",
        "        self.num_features = self.backbone.num_features\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(self.num_features, num_classes)\n",
        "        )\n",
        "        self._init_classifier()\n",
        "    \n",
        "    def _init_classifier(self):\n",
        "        for m in self.classifier.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.classifier(self.backbone(x))\n",
        "    \n",
        "    def get_params_groups(self, lr, backbone_lr_mult=0.1):\n",
        "        return [\n",
        "            {'params': self.backbone.parameters(), 'lr': lr * backbone_lr_mult},\n",
        "            {'params': self.classifier.parameters(), 'lr': lr}\n",
        "        ]\n",
        "\n",
        "def create_model(model_name='tf_efficientnetv2_s', num_classes=7, pretrained=True, dropout=0.3):\n",
        "    return EmotionEfficientNet(model_name, num_classes, pretrained, dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset_code"
      },
      "outputs": [],
      "source": [
        "%%writefile src/data/dataset.py\n",
        "\"\"\"Dataset loader\"\"\"\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "EMOTION_LABELS = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, data_dir, split='train', transform=None):\n",
        "        self.transform = transform\n",
        "        self.samples = self._load(Path(data_dir) / split)\n",
        "        print(f'Loaded {len(self.samples)} {split} samples')\n",
        "    \n",
        "    def _load(self, split_dir):\n",
        "        samples = []\n",
        "        for emotion_dir in sorted(split_dir.iterdir()):\n",
        "            if not emotion_dir.is_dir():\n",
        "                continue\n",
        "            label = self._get_label(emotion_dir.name)\n",
        "            for ext in ['*.jpg', '*.png']:\n",
        "                for img_path in emotion_dir.glob(ext):\n",
        "                    samples.append((str(img_path), label))\n",
        "        return samples\n",
        "    \n",
        "    def _get_label(self, name):\n",
        "        mapping = {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n",
        "        return mapping.get(name.lower(), 0)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "transforms_code"
      },
      "outputs": [],
      "source": [
        "%%writefile src/data/transforms.py\n",
        "\"\"\"Data augmentation\"\"\"\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import numpy as np\n",
        "\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "def get_train_transforms(img_size=224):\n",
        "    return A.Compose([\n",
        "        A.Resize(img_size, img_size),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.15, rotate_limit=15, p=0.5),\n",
        "        A.OneOf([A.MotionBlur(p=1), A.GaussianBlur(p=1)], p=0.3),\n",
        "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "        A.CoarseDropout(max_holes=8, max_height=img_size//10, max_width=img_size//10, p=0.3),\n",
        "        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "def get_val_transforms(img_size=224):\n",
        "    return A.Compose([\n",
        "        A.Resize(img_size, img_size),\n",
        "        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "class AlbumentationsWrapper:\n",
        "    def __init__(self, transform):\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __call__(self, image):\n",
        "        if hasattr(image, 'convert'):\n",
        "            image = np.array(image.convert('RGB'))\n",
        "        return self.transform(image=image)['image']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utils_code"
      },
      "outputs": [],
      "source": [
        "%%writefile src/utils/training_utils.py\n",
        "\"\"\"Training utilities\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class ExponentialMovingAverage:\n",
        "    def __init__(self, model, decay=0.9999):\n",
        "        self.decay = decay\n",
        "        self.shadow = {n: p.data.clone() for n, p in model.named_parameters() if p.requires_grad}\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def update(self, model):\n",
        "        for n, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.shadow[n] = self.decay * self.shadow[n] + (1 - self.decay) * p.data\n",
        "    \n",
        "    def apply_shadow(self, model):\n",
        "        self.backup = {n: p.data.clone() for n, p in model.named_parameters() if p.requires_grad}\n",
        "        for n, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                p.data = self.shadow[n]\n",
        "    \n",
        "    def restore(self, model):\n",
        "        for n, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                p.data = self.backup[n]\n",
        "\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "    \n",
        "    def forward(self, pred, target):\n",
        "        n_class = pred.size(1)\n",
        "        if target.dim() == 1:\n",
        "            target = torch.nn.functional.one_hot(target, n_class).float()\n",
        "        target = target * (1 - self.smoothing) + self.smoothing / n_class\n",
        "        return -(target * torch.nn.functional.log_softmax(pred, 1)).sum(1).mean()\n",
        "\n",
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.val = self.avg = self.sum = self.count = 0\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'f1_macro': f1_score(y_true, y_pred, average='macro'),\n",
        "        'f1_weighted': f1_score(y_true, y_pred, average='weighted')\n",
        "    }\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names, save_path):\n",
        "    cm = confusion_matrix(y_true, y_pred).astype('float')\n",
        "    cm = cm / (cm.sum(axis=1)[:, np.newaxis] + 1e-6)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.ylabel('True'); plt.xlabel('Predicted'); plt.title('Confusion Matrix')\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_section"
      },
      "source": [
        "## ðŸš€ Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/emotion_detection_v2')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "from src.models.efficientnet import create_model\n",
        "from src.data.dataset import EmotionDataset, EMOTION_LABELS\n",
        "from src.data.transforms import get_train_transforms, get_val_transforms, AlbumentationsWrapper\n",
        "from src.utils.training_utils import *\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'âœ… Using {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    'img_size': 224,\n",
        "    'batch_size': 64,  # Reduce to 32 if GPU OOM\n",
        "    'epochs': 50,      # Try 30 for faster testing\n",
        "    'lr': 1e-3,\n",
        "    'weight_decay': 1e-4,\n",
        "    'label_smoothing': 0.1,\n",
        "    'ema_decay': 0.9995,\n",
        "    'warmup_epochs': 5,\n",
        "}\n",
        "\n",
        "print('Configuration:', CONFIG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_data"
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "train_dataset = EmotionDataset('data', 'train', AlbumentationsWrapper(get_train_transforms(CONFIG['img_size'])))\n",
        "# Try 'test' or 'Test' depending on your folder structure\n",
        "val_dataset = EmotionDataset('data', 'test', AlbumentationsWrapper(get_val_transforms(CONFIG['img_size'])))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f'Train: {len(train_dataset)}, Val: {len(val_dataset)}, Batches: {len(train_loader)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_model"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = create_model().to(device)\n",
        "criterion = LabelSmoothingCrossEntropy(CONFIG['label_smoothing'])\n",
        "optimizer = torch.optim.AdamW(model.get_params_groups(CONFIG['lr'], 0.1), weight_decay=CONFIG['weight_decay'])\n",
        "\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
        "warmup = LinearLR(optimizer, start_factor=0.1, total_iters=CONFIG['warmup_epochs']*len(train_loader))\n",
        "cosine = CosineAnnealingLR(optimizer, T_max=(CONFIG['epochs']-CONFIG['warmup_epochs'])*len(train_loader), eta_min=1e-6)\n",
        "scheduler = SequentialLR(optimizer, [warmup, cosine], [CONFIG['warmup_epochs']*len(train_loader)])\n",
        "\n",
        "ema = ExponentialMovingAverage(model, CONFIG['ema_decay'])\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "print(f'âœ… Model: {sum(p.numel() for p in model.parameters())/1e6:.1f}M params')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_functions"
      },
      "outputs": [],
      "source": [
        "# Training functions\n",
        "def train_epoch(model, loader, criterion, optimizer, scheduler, scaler, ema):\n",
        "    model.train()\n",
        "    loss_meter = AverageMeter()\n",
        "    for images, labels in tqdm(loader, desc='Training'):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, labels)\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "        ema.update(model)\n",
        "        loss_meter.update(loss.item(), images.size(0))\n",
        "    return loss_meter.avg\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    for images, labels in tqdm(loader, desc='Validating', leave=False):\n",
        "        images = images.to(device)\n",
        "        logits = model(images)\n",
        "        all_preds.extend(logits.argmax(1).cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "    return compute_metrics(np.array(all_labels), np.array(all_preds)), np.array(all_preds), np.array(all_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_loop"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "best_f1 = 0\n",
        "history = {'train_loss': [], 'val_f1': [], 'ema_val_f1': []}\n",
        "\n",
        "for epoch in range(CONFIG['epochs']):\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer, scheduler, scaler, ema)\n",
        "    val_metrics, _, _ = validate(model, val_loader)\n",
        "    \n",
        "    ema.apply_shadow(model)\n",
        "    ema_metrics, _, _ = validate(model, val_loader)\n",
        "    ema.restore(model)\n",
        "    \n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_f1'].append(val_metrics['f1_macro'])\n",
        "    history['ema_val_f1'].append(ema_metrics['f1_macro'])\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{CONFIG['epochs']}: Loss={train_loss:.4f}, F1={val_metrics['f1_macro']:.4f}, EMA F1={ema_metrics['f1_macro']:.4f}\")\n",
        "    \n",
        "    if ema_metrics['f1_macro'] > best_f1:\n",
        "        best_f1 = ema_metrics['f1_macro']\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'ema_shadow': ema.shadow,\n",
        "            'best_f1': best_f1\n",
        "        }, 'models/best_model.pth')\n",
        "        print(f'  âœ… Saved best model (F1={best_f1:.4f}) to Drive!')\n",
        "\n",
        "print(f'\\nâœ… Training complete! Best F1: {best_f1:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eval_section"
      },
      "source": [
        "## ðŸ“Š Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_eval"
      },
      "outputs": [],
      "source": [
        "# Load best model\n",
        "checkpoint = torch.load('models/best_model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "for n, p in model.named_parameters():\n",
        "    if n in checkpoint['ema_shadow']:\n",
        "        p.data = checkpoint['ema_shadow'][n]\n",
        "\n",
        "test_metrics, test_preds, test_labels = validate(model, val_loader)\n",
        "\n",
        "print('\\n' + '='*50)\n",
        "print('FINAL TEST RESULTS')\n",
        "print('='*50)\n",
        "print(f\"Accuracy: {test_metrics['accuracy']:.4f} ({test_metrics['accuracy']*100:.2f}%)\")\n",
        "print(f\"F1 Macro: {test_metrics['f1_macro']:.4f}\")\n",
        "print(f\"F1 Weighted: {test_metrics['f1_weighted']:.4f}\")\n",
        "\n",
        "with open('results/test_results.txt', 'w') as f:\n",
        "    f.write(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\\n\")\n",
        "    f.write(f\"F1 Macro: {test_metrics['f1_macro']:.4f}\\n\")\n",
        "    f.write(f\"F1 Weighted: {test_metrics['f1_weighted']:.4f}\\n\")\n",
        "\n",
        "print('\\nâœ… Results saved to Drive!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "confusion_matrix"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "class_names = [EMOTION_LABELS[i] for i in range(7)]\n",
        "plot_confusion_matrix(test_labels, test_preds, class_names, 'results/confusion_matrix.png')\n",
        "\n",
        "from IPython.display import Image, display\n",
        "display(Image('results/confusion_matrix.png'))\n",
        "print('âœ… Confusion matrix saved!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_curves"
      },
      "outputs": [],
      "source": [
        "# Training curves\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
        "ax[0].plot(history['train_loss'])\n",
        "ax[0].set_title('Training Loss')\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[1].plot(history['val_f1'], label='Val')\n",
        "ax[1].plot(history['ema_val_f1'], label='EMA Val')\n",
        "ax[1].set_title('F1 Score')\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].legend()\n",
        "plt.savefig('results/training_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print('âœ… Training curves saved!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "export_section"
      },
      "source": [
        "## ðŸ’¾ Export to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onnx_export"
      },
      "outputs": [],
      "source": [
        "# Export to ONNX\n",
        "model.eval()\n",
        "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "\n",
        "torch.onnx.export(\n",
        "    model, dummy_input, 'models/emotion_model.onnx',\n",
        "    input_names=['input'], output_names=['output'],\n",
        "    dynamic_axes={'input': {0: 'batch'}, 'output': {0: 'batch'}}\n",
        ")\n",
        "\n",
        "print('âœ… ONNX model exported!')\n",
        "print('\\nðŸ“‚ Files in your Drive:')\n",
        "!ls -lh /content/drive/MyDrive/Emotion_Detection_v2/models/\n",
        "!ls -lh /content/drive/MyDrive/Emotion_Detection_v2/results/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## âœ… Complete!\n",
        "\n",
        "**Saved to Google Drive** (`/Emotion_Detection_v2/`):\n",
        "- âœ… `models/best_model.pth` - Best checkpoint\n",
        "- âœ… `models/emotion_model.onnx` - ONNX export\n",
        "- âœ… `results/confusion_matrix.png`\n",
        "- âœ… `results/training_curves.png`\n",
        "- âœ… `results/test_results.txt`\n",
        "\n",
        "Download these from your Drive for local use!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
